# LLM을 활용한 실전 AI 애플리케이션 개발
## 01 LLM 지도
2022년말 Chat GPT가 등장했고 사용자와 대화가 된다고 느껴질 정도로 놀라운 성능을 보였다. 

하지만 그런 놀라운 성능에도 불구하고 GPT는 굉장히 단순한 과정으로 동작하는데 1장에서는 LLM의 기반이 되는 딥러닝과 언어 모델링에 대해 알아본다.

### 1.1 딥러닝과 언어 모델링
LLM은 **딥러닝** 기반의 **언어 모델**이다.

* 딥러닝 : 데이터의 패턴을 학습한느 머신러닝의 한 분야, 표 형태의 정형 데이터 뿐만 아니라 텍스트와 이미지 같은 비정형 데이터에서도 뛰어난 패턴 인식 성능을 보임.
* 언어 모델 : 다음에 올 단어를 예측하는 모델

이러한 LLM이 지금처럼 자리 잡기까지 중요했던 세 가지 사건이 있다. 
1. 2013년 구글에서 **워드투벡** 발표.
2. 2017년 구글에서 **트랜스포머 아키텍쳐** 공개
3. 2018년 OpenAI의 **GPT-1 모델** 공개 

이 세 가지 주요 사건들이 어떠한 영향들을 미쳐왔는지 알아보자.

---
### 1.1.1 데이터의 특징을 스스로 추출하는 딥러닝

딥러닝의 단순하면서도 범용적으로 문제를 해결하는 3단계 접근 방식
+ 문제의 유형(예: 자연어 처리, 이미지 처리)에 따라 일반적으로 사용되는 모델을 준비
+ 풀고자 하는 문제에 대한 학습 데이터를 준비
+ 학습 데이터를 반복적으로 모델에 입력

딥러닝은 기존 머신러닝과 다르게 연구자 또는 개발자가 데이터의 특징을 뽑지않고 모델이 **스스로** 데이터의 특징을 찾고 분류하는 모든 과정을 학습한다.

---

### 1.1.2 임베딩: 딥러닝 모델이 데이터를 표현하는 방식

딥러닝 모델은 데이터의 의미를 숫자의 집합으로 표현한다. 

이때 데이터의 의미와 특징을 포착해 그 의미를 담고 여러 개의 숫자 집합으로 표현하는 것을 **임베딩(Embedding)** 이라고 부른다.    
데이터를 임베딩으로 표현하면 데이터 사이의 거리를 계산하고 거리를 바탕으로 관련 있는 데이터와 관련이 없는 데이터를 구분할 수 있다.   
#### 이때 2013년 구글에서 워드투벡(word2vec)이라는 모델을 통해 단어를 임베딩(여러개의 숫자 집합)으로 변환하는 방법을 소개했다.   
단어를 임베딩으로 변환한 것을 단어 임베딩(word embedding)이라고 한다.   
(단어를 임베딩으로 변환하다 = 데이터(단어)들을 의미와 특징을 담아 여러개의 숫자 집합으로 표현하다)   

* 다음은 요즘 유행하는 MBTI를 임베딩 모델에 비교해 표로 나타낸 것이다.   
![image](https://github.com/user-attachments/assets/f1fa473c-3ad7-4147-9b14-5b4e504f06ad)

---

### 1.1.3 언어 모델링: 딥러닝 모델의 언어 학습법

* 언어 모델링 : 모델이 입력받은 텍스트의 다음 단어를 예측해 텍스트를 생성하는 방식.   대량의 데이터에서 언어의 특성을 학습하는 사전 학습(pre-training)과제로도 많이 사용된다.

기존의 머신러닝 모델 학습은 지도 학습(supervised-learning) 방식으로 각각의 데이터 셋으로 별도의 모델을 학습 시켰다.    
이에 발전해 지금의 딥러닝 분야에서는 총 **두 단계** 로 나눠 학습을 진행한다. 그리고 이 과정을 전이 학습(transfer-learning)이라고 부른다.


* 사전 학습(pre-training) : 대량의 데이터로 모델을 학습 시킴. 이때 대량의 데이터는 꼭 문제를 해결하기 위한 데이터와 관련이 있는 것은 아니다.
* 미세 조정(fine-tunning) : 특정한 문제를 해결하기 위한 데이터로 추가 학습하는 것. (사전 학습모델을 미세 조정해 풀고자 하는 과제를 다운스트림(downtream) 과제라고 부른다.)

![image](https://github.com/user-attachments/assets/8afb45e7-5d8b-4247-9d20-98d79d9ace70)

이때 재밌는 것은 학습시키려는 데이터와 다른 대량의 데이터들로 사전 학습을 시킨 것이 결국 점이나 선같은 특징을 파악하는데 도움을 주기 때문에 일반적으로 성능이 더 좋아졌다.

--- 

## 1.2 (딥러닝)언어 모델이 CHAT GPT가 되기 까지

![image](https://github.com/user-attachments/assets/baaec957-8282-4143-9610-ce2cb9ab0b95)

--- 

### 1.2.1 RNN에서 트랜스포머 아키텍처(구조)로

RNN(순환 신경망, Recurrent Neural Network): 트랜스포머 아키텍처를 사용하기 전 텍스트를 생성하기 위해 사용되었던 아키텍처
![image](https://github.com/user-attachments/assets/27c6d24a-009f-42bc-a62e-e4bf651231c4) 

- 입력하는 텍스트를 **순차적** 으로 처리해서 다음 단어를 예측 -> 먼저 입력한 단어의 의미가 점차 희석되며, 입력이 길어지는 경우 성능 저하
- 모델이 '하나의 잠재 상태(hidden state)'에 지금까지의 입력 텍스트의 맥락을 압축 -> 메모리를 적게 사용, 다음 단어를 빠르게 생성
  
트랜스 포머 아키텍처: RNN의 **순차적인** 방식을 버리고, 맥락을 모두 참조하는 **어텐션(atttention)** 연산을 사용 -> RNN의 문제를 대부분 해결
![image](https://github.com/user-attachments/assets/34984f01-7cd6-414f-9225-13b1abce4e15)

- 맥락을 압축하지 않고 그대로 활용하기 때문에 성능을 높일 수 있지만, 입력 텍스트가 길어지면 맥락 데이터를 모두 저장하고 있어야 하기 때문에 메모리 사용량이 증가
- 매번 단어를 예측할 때마다 맥락 데이터를 모두 확인해야 하기 때문에 입력이 길어지면 예측에 걸리는 시간도 증가
- **but 성능이 좋고 병렬 처리를 통해 학습 속도를 높일 수 있어 현재는 대부분 LLM이 트랜스포머 아키텍처를 기반으로 한다.**
  
### 1.2.2 GPT 시리즈로 보는 모델 크기와 성능의 관계 

언어 모델이 학습하는 과정 = 언어 모델이 학습 데이터를 압축하는 과정   
이때 압축은 공통되고 중요한 패턴을 남기는 손실 압축이다.    
중요한 점은 모델이 계속해서 커진다고 성능이 높아지지 않고 학습 데이터의 크기가 최대 모델 크기의 상한이라고 볼 수 있다.

---

### 1.2.3 Chat GPT의 등장

GPT-3를 CHAT CPT로 바꾼것은 OpenAI가 논문의 연구 결과 발표와 함께 공개한 **지도 미세 조정(supervised-fine-tunning)** 과 **RLHF(Reinforcement Learning from Human Feedback)** 라는 기술 덕분이었다.   
이 기술들을 통해 챗GPT는 그저 사용자가 한 말 다음에 이어질 말을 생성하는 것이 아니라 사용자의 요청을 해결할 수 있는 텍스트를 생성하게 되었다.   

* 지도 미세 조정: 정렬(LLM이 생성하는 답변을 사용자의 요청 의도에 맞추는 것)을 위한 가장 핵심적인 학습 과정으로서, 언어 모델링으로 사전 학습한 언어 모델을 지시 데이터셋으로 추가 학습하는 것.
이때 지시 데이터셋은 사용자가 요청 또는 지시한 사항과 그에 대한 적절한 응답을 정리한 데이터 셋.
* RLHF: OpenAI에서는 두 가지 답변 중 사용자가 더 선호하는 답변을 선택한 데이터셋을 구축, 이를 선호 데이터셋이라고 한다. 이 선호 데이터셋으로 LLM의 답변을 평가하는 리워드 모델을 만들고 LLM이 점점 더 높은 점수를 받을 수 있도록 추가 학습하는 것.

--- 

## 1.3 LLM 애플리케이션의 시대가 열린다

![image](https://github.com/user-attachments/assets/24a1b9c5-2e5f-45f0-8a9d-bfd8a5abb623)

---

### 1.3.1 지식 사용법을 획기적으로 바꾼 LLM 

기존의 자연어 처리 접근 방식에서는 언어 이해 모델과 언어 생성 모델을 각각 개발해 연결했다. 때문에 시스템 복잡도가 높았고 관리가 어려웠다.    
하지만 LLM의 경우 언어 이해와 생성 능력이 모두 뛰어나 **다재다능** 하다. 

![image](https://github.com/user-attachments/assets/e890450c-fc6a-4953-85d6-948108dd92c1)

LLM은 우리가 기존에 지식을 습득하고 활용하던 모든 측면에 영향을 줄 수 있기 때문에 이전의 AI 모델보다 사회에 미치는 영향이 크다.
![image](https://github.com/user-attachments/assets/d669e016-b741-47e1-b94a-a3a46ffdefb5)

---

### 1.3.2 sLLM: 더 작고 효율적인 모델 만들기 

* LLM을 활용하는 두 가지 방법
1. OpenAI의 GPT-4나 구글의 제미나이처럼 **상업용 API**를 사용하는 방법 -> 모델이 크고 범용 텍스트 생성 능력이 뛰어남
2. 오픈소스 LLM을 활용해 **직접 LLM API**를 생성해 사용하는 방법 -> 작업을 위한 데이터를 자유롭게 추가 학습 가능 

* 오픈소스 LLM을 활용해 추가 학습을 하는 경우 모델 크기가 작으면서도 특정 도메인 데이터나 작업에서 높은 성능을 보이는 모델을 만들 수 있는데, 이를 **sLLM**이라고 한다.

--- 

### 1.3.3 더 효율적인 학습과 추론을 위한 기술

LLM의 기반이 되는 트랜스포머 아키텍처 연산은 LLM의 학습과 추론에 필요한 연산량이 크게 증가했다. 이 많은 연산량을 빠르게 처리하기 위해 많은 연산을 병렬로 처리하는데 특화된 처리 장치인 GPU를 사용한다.    
하지만 요즘 비용도 비용이지만 챗GPT 수요가 급증하며 돈을 주고도 GPU을 구하지 못하는 품귀 현상도 나타나고 있다.   
때문에 적은 GPU자원으로도 LLM을 활용할 수 있또록 돕는 연구가 진행중이며 대표적으로 모델 파라미터를 더 적은 비트로 표현하는 양자화와 모델전체를 학습하는 것이 아니라 모델의 일부만 학습하는 LoRA방식이 있다.

--- 

### 1.3.4 LLM의 환각 현상을 대처하는 검색 증각 생성(RAG) 기술

* 환각 현상 : LLM의 한 가지 큰 문제, LLM이 잘못된 정보나 실제로 존재하지 않는 정보를 만들어 내는 현상.
> 이유는 정확히 알기 어렵지만 사실 LLM이 특정 정보가 사실인지 판단할 능력이 없다. 또한 학습 데이터를 압축하는 과정에서 비교적 드물게 등장하는 정보는 소실되는데, 그런 정보의 소실이 부정확한 정보를 생성하는 원인이 될 수도 있다.

* RAG(검색 증강 생성, Retrival Augmented Generation) : 환각 현상을 줄이기 위한 기술, 프롬프트에 LLM이 답변할 때 필요한 정보를 미리 추가함으로써 잘못된 정보를 생성하는 문제를 줄인다.

--- 

## 1.4 LLM의 미래 : 인식과 행동의 확장 

![image](https://github.com/user-attachments/assets/a34a5543-98f8-4ba8-a246-914a21274559)   

앞으로의 LLM이 어떤 방향으로 발전할까에 대한 세 가지 큰 흐름이다. 

먼저, LLM이 더 다양한 형식의 데이터를 입력으로 받을 수 있고 출력으로도 여러 형태의 데이터를 생성할 수 있도록 발전시킨 멀티 모달 LLM이 있다. 또 LLM이 텍스트 생성 능력을 사용해 계획을 세우거나 의사결정을 내리고 필요한 행동까지 수행하는 에이전트 연구도 활발히 진행되고 있다. LLM이 사용하는 트랜스포머 아키텍처를 새로운 아키텍처로 변경해 더 긴 입력을 효율적으로 처리하려는 연구도 주목받고 있다. 

> 멀티 모달 모델이란 다양한 형태의 입력을 받을 수 있는 LLM을 말한다.
![image](https://github.com/user-attachments/assets/083eeef7-aa1c-46cc-ac98-69296c36c199)

--- 

## 02 LLM의 중추, 트랜스포머 아키텍처 살펴보기

**이번 장에서의 목표는 트랜스포머 아키텍처를 코드 레벨에서 직접 구현해 보면서 세부적인 동작을 이해해 보는 것이다.**

---

## 2.1 트랜스포머 아키텍처란

기존 RNN은 1장에서 배웠다싶이 텍스트를 순차적으로 하나씩 입력하는 형태였다.    
![image](https://github.com/user-attachments/assets/71b9bfa3-10cc-48cf-984b-7d75e6ad1cce)   

위 그림에서 x는 텍스트 토큰(token)이고 이는 거의 모든 자연어 처리 연산의 기본 단위이자, 보통 단어보다 짧은 텍스트 단위다. (단어와 같다고 생각하자)   
h는 입력 토큰을 RNN 모델에 입력했을 때의 출력인데, 그림에서 확인할 수 있듯이 이전 토큰의 출력을 다시 모델에 입력으로 사용하기 때문에 입력을 병렬적으로 처리하지 못하는 구조다.    
이런 구조 때문에 학습 속도가 느리고, 입력이 길어졌을때 먼저 입력한 토큰의 정보가 희석되면서 성능이 떨어진다는 문제가 있는 것이다. 성능을 높이기 위해 층을 깊이 쌓으면 그레디언트 소실(gradient vanishing)이나 그레디언트 증폭(gradient exploding)이 발생해 학습이 불안정했다.

반면 트랜스포머는 이런 RNN의 문제를 해결하고자 셀프 어텐션(self-attention)이라는 개념을 도입한다.    
> 셀프 어텐션(self-attention) : 입력된 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산해서 각 단어의 표현을 조정하는 역할. 이로인해 트랜스포머는 다음과 같은 장점을 보임

![image](https://github.com/user-attachments/assets/d9c61a74-2a3b-444b-b15f-8b1266d9b17d)

다음으로 가장 중요한 전체 트랜스포머 아키텍처를 살펴보자.   

![image](https://github.com/user-attachments/assets/8bd1037e-62ae-4dbb-9dbf-1ec32560872e)

트랜스포머 아키텍처는 크게 인코더와 디코더로 나뉜다. 인코더는 언어를 이해하는 역할, 디코더는 언어를 생성하는 역할을 한다. 공통적으로 입력을 임베딩층을 통해 숫자 집합인 임베딩으로 변환하고 위치 인코딩(positional encoding)층에서 문장의 위치 정보를 더한다.   
인코더에서는 층 정규화(layer normalization), 멀티 헤드 어텐션(multi-head-attention), 피드 포워드(feed forward)층을 거치며 영어 문장을 이해하고 디코더로 전달한다.   
디코더에서는 인코더와 유사하게 층 정규화, 멀티 헤드 어텐션 연산을 수행하면서 크로스 어텐션 연산을 통해 인코더가 전달한 데이터를 출력과 함께 종합해서 피드 포워드 층을 거쳐 한국어 번역 결과를 생성한다.

---

## 2.2 텍스트를 임베딩으로 변환하기

컴퓨터는 텍스트를 그대로 계산에 사용할 수 없어 숫자 형식의 데이터로 변경해야한다.   

![image](https://github.com/user-attachments/assets/17b1e8d7-dea5-4369-928c-c9bf2f1d4f0b)

먼저 텍스트를 적절한 단위로 잘라 숫자형 ID를 부여하는 토큰화(tokenization)을 수행하고 토큰 임베딩 층, 위치 인코딩 층을 통해 여러 숫자의 집합인 토큰임베딩, 토큰의 위치 정보를 담고 있는 위치 임베딩을 추가해 최종적으로 모델에 입력할 임베딩을 만들게 된다. 

이 세가지 과정에 대해 자세히 알아보자.

---

### 2.2.1 토큰화 

토큰화란 텍스트를 적절한 단위로 나누고 숫자 ID를 부여하는 것을 말한다. 단위의 기준은 작게는 자모, 크게는 단어 단위로 나눌 수 있고 어떤 토큰이 어떤 숫자 ID로 연결 됐는지 기록한 사전을 만들어야 한다. 

![image](https://github.com/user-attachments/assets/8a0e7212-4b4f-45d4-8cee-89cbee86b7a7)
> OOV(Out Of Vocabulary) : 사전에 없는 단어

위와 같이 작은 단위, 큰 단위 모두 각각 장단점이 뚜렷해 최근에는 데이터에 등장하는 빈도에 따라 토큰화 단위를 결정하는 **서브워드(subword) 토큰화** 방식을 사용한다. 

![image](https://github.com/user-attachments/assets/5bcac72c-3222-47e7-ada6-22ce4c666ee1)

서브워드 토큰화 방식에서는 자주 나오는 단어는 단어 단위 그대로 유지하고 가끔 나오는 단어는 더 작은 단위로 나눠 텍스트의 의미를 최대한 유지하면서 사전의 크기는 작고 효율적으로 유지할 수 있다.